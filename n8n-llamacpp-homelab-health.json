[
  {
    "name": "Homelab Health with llama.cpp",
    "nodes": [
      {
        "parameters": {},
        "id": "Manual Trigger",
        "name": "Manual Trigger",
        "type": "n8n-nodes-base.manualTrigger",
        "typeVersion": 1,
        "position": [250, 300]
      },
      {
        "parameters": {
          "command": "df --output=pcent / | tail -1 | tr -dc '0-9'",
          "options": {}
        },
        "id": "Disk Usage",
        "name": "Disk Usage",
        "type": "n8n-nodes-base.executeCommand",
        "typeVersion": 1,
        "position": [450, 200]
      },
      {
        "parameters": {
          "command": "free | awk '/Mem:/ {print int($3/$2 * 100.0)}'",
          "options": {}
        },
        "id": "Memory Usage",
        "name": "Memory Usage",
        "type": "n8n-nodes-base.executeCommand",
        "typeVersion": 1,
        "position": [450, 300]
      },
      {
        "parameters": {
          "command": "systemctl --failed --no-legend | wc -l",
          "options": {}
        },
        "id": "Failed Services",
        "name": "Failed Services",
        "type": "n8n-nodes-base.executeCommand",
        "typeVersion": 1,
        "position": [450, 400]
      },
      {
        "parameters": {
          "command": "systemctl is-active docker || echo inactive",
          "options": {}
        },
        "id": "Docker Running",
        "name": "Docker Running",
        "type": "n8n-nodes-base.executeCommand",
        "typeVersion": 1,
        "position": [650, 200]
      },
      {
        "parameters": {
          "command": "systemctl is-active ollama || echo inactive",
          "options": {}
        },
        "id": "Ollama Running",
        "name": "Ollama Running",
        "type": "n8n-nodes-base.executeCommand",
        "typeVersion": 1,
        "position": [650, 300]
      },
      {
        "parameters": {
          "functionCode": "// Parse all metrics and build the prompt\nconst disk = parseInt(items[0].json.stdout.trim(), 10);\nconst mem = parseInt(items[1].json.stdout.trim(), 10);\nconst failed = parseInt(items[2].json.stdout.trim(), 10);\nconst docker_running = items[3].json.stdout.trim() === 'active';\nconst ollama_running = items[4].json.stdout.trim() === 'active';\n\nconst metrics = {\n  disk_usage_percent: disk,\n  memory_usage_percent: mem,\n  failed_services_count: failed,\n  docker_running,\n  ollama_running\n};\n\nconst prompt = `You are a simple AI homelab system administrator. You must analyze these exact metrics and respond with JSON only.\n\nCURRENT SYSTEM STATE:\n- Disk usage: ${metrics.disk_usage_percent}% (CRITICAL if >90%)\n- Memory usage: ${metrics.memory_usage_percent}%\n- Failed services: ${metrics.failed_services_count} (BAD if >0)\n- Docker running: ${metrics.docker_running}\n- Ollama running: ${metrics.ollama_running}\n\nDECISION RULES:\n- If disk >90%: ALWAYS do docker_cleanup AND journal_cleanup\n- If failed services >0: ALWAYS do service_restart  \n- If disk >95%: Set urgency to \"immediate\"\n- If services down: ALWAYS restart them\n\nAVAILABLE ACTIONS (pick from these exact names):\n- package_update: Updates system packages\n- docker_cleanup: Removes unused Docker data  \n- journal_cleanup: Cleans old system logs\n- service_restart: Restarts Docker and Ollama\n- system_check: Basic health check\n- ai_cleanup: Cleans AI model storage\n\nRespond with ONLY this JSON format (no explanation):\n{\n    \"analysis\": \"1-2 sentence summary\",\n    \"actions\": [\n        {\"command\": \"exact_action_name\", \"reason\": \"brief reason\", \"priority\": \"high\"}\n    ],\n    \"urgency\": \"immediate\"\n}\n\nSystem metrics: ${JSON.stringify(metrics, null, 2)}`;\n\nreturn [{json: {prompt, metrics}}];"
        },
        "id": "Build Prompt",
        "name": "Build Prompt",
        "type": "n8n-nodes-base.function",
        "typeVersion": 1,
        "position": [850, 300]
      },
      {
        "parameters": {
          "requestMethod": "POST",
          "url": "http://localhost:8080/completion",
          "jsonParameters": true,
          "options": {},
          "bodyParametersJson": "{\"prompt\": \"={{$json[\"prompt\"]}}\"}"
        },
        "id": "llama.cpp API",
        "name": "llama.cpp API",
        "type": "n8n-nodes-base.httpRequest",
        "typeVersion": 1,
        "position": [1050, 300]
      }
    ],
    "connections": {
      "Manual Trigger": [
        ["Disk Usage"],
        ["Memory Usage"],
        ["Failed Services"]
      ],
      "Disk Usage": [["Docker Running"]],
      "Memory Usage": [["Ollama Running"]],
      "Failed Services": [["Build Prompt"]],
      "Docker Running": [["Build Prompt"]],
      "Ollama Running": [["Build Prompt"]],
      "Build Prompt": [["llama.cpp API"]]
    },
    "active": false,
    "settings": {},
    "id": "llamacpp-homelab-health"
  }
]